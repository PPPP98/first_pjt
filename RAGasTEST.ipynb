{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45c3fa7c",
   "metadata": {},
   "source": [
    "# RAGAS TEST\n",
    "## 1. VectorDB 비교\n",
    "#### FAISS-cpu vs ChromaDB\n",
    "#### 선정 이유\n",
    "- 접근성\n",
    "- 로컬 저장\n",
    "- 성능\n",
    "- 커뮤니티"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7930c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain langchain-openai langchain-chroma langchain-community openai faiss-cpu python-docx dotenv rank_bm25 ragas transformers sentence_transformers matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de40358",
   "metadata": {},
   "source": [
    "### 환경 변수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163615e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "file_path = os.getenv(\"FILE_PATH\")\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ[\"OPENAI_API_BASE\"] = os.getenv(\"OPENAI_API_BASE\")\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a5c87a",
   "metadata": {},
   "source": [
    "## RAGAS 평가 코드 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055b60d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import (\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    context_recall,\n",
    "    context_precision,\n",
    ")\n",
    "import json\n",
    "from pprint import pprint\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-small\",\n",
    ")\n",
    "\n",
    "QA_set = []\n",
    "\n",
    "# GT 불러오기\n",
    "try:\n",
    "    if os.path.exists(\"ground_truths.json\"):\n",
    "        with open(\"ground_truths.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "            ground_truths = json.load(f)\n",
    "        print(\"✅ GT 로드 완료\")\n",
    "    else:\n",
    "        raise FileNotFoundError\n",
    "\n",
    "    for doc in ground_truths:\n",
    "        QA_set.append(doc['qa_pairs'])\n",
    "    else:\n",
    "        print(\"Q&A set 생성 완료\")\n",
    "except FileNotFoundError:\n",
    "    print(\"❌ GT 파일이 존재하지 않음\")\n",
    "\n",
    "pprint(QA_set[:3])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337e4c7c",
   "metadata": {},
   "source": [
    "# VectorDB\n",
    "- precision\n",
    "- recall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ebeeb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_chroma.vectorstores import Chroma\n",
    "\n",
    "faiss_store = FAISS.load_local(\"faiss_store\", embeddings=embeddings, allow_dangerous_deserialization=True) if os.path.exists(\"faiss_store\") else None\n",
    "chroma_store = Chroma(persist_directory=\"chroma_store\", embedding_function=embeddings) if os.path.exists(\"chroma_store\") else None\n",
    "all_texts = None\n",
    "\n",
    "if faiss_store is None:\n",
    "    print(\"❌FAISS 파일 없음\")\n",
    "elif chroma_store is None:\n",
    "    print(\"❌ChromaDB 파일 없음\")\n",
    "else:\n",
    "    print(\"✅벡터 DB 로드 완료\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ff9d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_types = [\"similarity \", \"mmr\"]\n",
    "sparse_types = [\"bm25\"]\n",
    "ensemble_types = [\"ensemble\"]\n",
    "\n",
    "retriever_types = [dense_types, sparse_types, ensemble_types]\n",
    "\n",
    "faiss_score = {}\n",
    "chroma_score = {}\n",
    "\n",
    "all_texts = [doc.page_content for doc in faiss_store.docstore._dict.values()]\n",
    "K_VALUE = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b725ce",
   "metadata": {},
   "source": [
    "## 함수 정의\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99201a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.retrievers import BM25Retriever\n",
    "from langchain.retrievers import EnsembleRetriever\n",
    "\n",
    "\"\"\"\n",
    "검색기 사전 정의, RAGAS 테스트 검색기 별  자동화 완료\n",
    "함수 : \n",
    "ragas_evaluate -> RAGAS 검색기 성능평가 함수\n",
    "\n",
    "fill_data -> data 채우기\n",
    "\n",
    "evaluate_retr -> 모든 검색기 자동화 저장 (test 완)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# 점수 계산 함수\n",
    "def ragas_evaluate(dataset):\n",
    "    result = evaluate(\n",
    "        dataset,\n",
    "        metrics=[\n",
    "            context_precision,\n",
    "            context_recall,\n",
    "        ],\n",
    "        llm=llm,\n",
    "        embeddings=embeddings,\n",
    "    )\n",
    "    return result\n",
    "\n",
    "# 데이터\n",
    "def fill_data(data, question, retr, ground_truth):\n",
    "\n",
    "    results = retr.invoke(question)\n",
    "    context = [doc.page_content for doc in results]\n",
    "\n",
    "    # llm 응답은 기록하지 않음\n",
    "\n",
    "    data[\"question\"].append(question)\n",
    "    data[\"answer\"].append(\"\")\n",
    "    data[\"contexts\"].append(context)\n",
    "    data[\"ground_truth\"].append(ground_truth)\n",
    "\n",
    "\n",
    "def evaluate_retr(all_retrievers_map, score, test=False):\n",
    "\n",
    "    for retr_name, retr in all_retrievers_map.items():\n",
    "        print(f\"/ {retr_name} / 검색 평가 시작\")\n",
    "        _data_frame = {\n",
    "            \"question\": [],\n",
    "            \"answer\": [],\n",
    "            \"contexts\": [],\n",
    "            \"ground_truth\": [],\n",
    "        }\n",
    "\n",
    "        if test:\n",
    "            print(f\"{retr_name} test\")\n",
    "            fill_data(_data_frame, QA_set[0][\"Q\"], retr, QA_set[0][\"A\"])\n",
    "\n",
    "        else: \n",
    "            for idx, qa in enumerate(QA_set):\n",
    "                fill_data(_data_frame, qa[\"Q\"], retr, qa[\"A\"])\n",
    "                print(f\"✅{idx + 1}/{len(QA_set)}\")\n",
    "\n",
    "        print(\"start\")\n",
    "        _dataset = Dataset.from_dict(_data_frame)\n",
    "        score[retr_name] = ragas_evaluate(_dataset)\n",
    "            \n",
    "    print(\"done\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20953364",
   "metadata": {},
   "source": [
    "# test start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60fc432b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bm25\n",
    "sparse_bm25_retriever = BM25Retriever.from_texts(texts=all_texts)\n",
    "sparse_bm25_retriever.k = K_VALUE  # k값을 통일하여 설정\n",
    "\n",
    "# BM25만 독립적으로 평가\n",
    "bm25_score_dict = {}\n",
    "evaluate_retr(\n",
    "    {\"bm25\": sparse_bm25_retriever}, # BM25만 담긴 맵을 전달\n",
    "    bm25_score_dict,\n",
    "    test=True\n",
    ")\n",
    "\n",
    "faiss_score [\"bm25\"] = bm25_score_dict[\"bm25\"]\n",
    "chroma_score [\"bm25\"] = bm25_score_dict[\"bm25\"]\n",
    "print(\"--- [ BM25 ] 평가 완료 ---\\n\")\n",
    "\n",
    "\n",
    "for db_name, vectordb, vectordb_score in [\n",
    "    (\"FAISS\", faiss_store, faiss_score),\n",
    "    (\"CHroma\", chroma_store, chroma_score),\n",
    "]:\n",
    "    # similarity\n",
    "    dense_similarity_retriever = vectordb.as_retriever(\n",
    "        search_type=\"similarity\", search_kwargs={\"k\": K_VALUE}\n",
    "    )\n",
    "    # mmr\n",
    "    dense_mmr_retriever = vectordb.as_retriever(\n",
    "        search_type=\"mmr\",\n",
    "        search_kwargs={\"k\": K_VALUE, \"fetch_k\": 20},  # MMR은 fetch_k를 추가로 설정 가능\n",
    "    )\n",
    "    # ensemble\n",
    "    ensemble_retriever = EnsembleRetriever(\n",
    "        retrievers=[sparse_bm25_retriever, dense_similarity_retriever],\n",
    "        weights=[0.5, 0.5],  # 가중치 설정\n",
    "    )\n",
    "\n",
    "    all_retrievers_map = {\n",
    "        \"similarity\": dense_similarity_retriever,\n",
    "        \"mmr\": dense_mmr_retriever,\n",
    "        # \"bm25\": sparse_bm25_retriever,\n",
    "        \"ensemble\": ensemble_retriever,\n",
    "    }\n",
    "\n",
    "\n",
    "    evaluate_retr(all_retrievers_map, vectordb_score, test=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d787fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "methods = [\"similarity\", \"mmr\", \"bm25\", \"ensemble\"]\n",
    "\n",
    "for score in [faiss_score, chroma_score]:\n",
    "\n",
    "    precision = [np.nanmean(score[type_name]['context_precision']) for type_name in methods]\n",
    "    recall = [np.nanmean(score[type_name]['context_recall']) for type_name in methods]\n",
    "\n",
    "    # 시각화\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    # Precision과 Recall을 나란히 보여주기 위해 bar width 설정\n",
    "    bar_width = 0.35\n",
    "    index = range(len(methods))\n",
    "\n",
    "    # Precision과 Recall Bar 생성\n",
    "    bar1 = ax.bar(index, precision, bar_width, label='Precision')\n",
    "    bar2 = ax.bar([i + bar_width for i in index], recall, bar_width, label='Recall')\n",
    "\n",
    "    # Label 및 제목 설정\n",
    "    ax.set_xlabel('Retrieval Methods')\n",
    "    ax.set_ylabel('Scores')\n",
    "    ax.set_title('Precision and Recall for Different Retrieval Methods')\n",
    "    ax.set_xticks([i + bar_width / 2 for i in index])\n",
    "    ax.set_xticklabels(methods)\n",
    "    ax.legend()\n",
    "\n",
    "    # 그래프 출력\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "print(\"---faiss---\")\n",
    "pprint(faiss_score)\n",
    "print(\"---chroma---\")\n",
    "pprint(chroma_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705fddb8",
   "metadata": {},
   "source": [
    "# reranking test\n",
    "\n",
    "### 최종 선택 : FAISS with ensemble\n",
    "\n",
    "### 1. ensemble 가중치 최적화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8281ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.retrievers import BM25Retriever\n",
    "from langchain.retrievers import EnsembleRetriever\n",
    "\n",
    "# 변수\n",
    "K_RE = 3\n",
    "# bm25\n",
    "sparse_bm25_retriever = BM25Retriever.from_texts(texts=all_texts)\n",
    "sparse_bm25_retriever.k = K_RE  # k값을 통일하여 설정\n",
    "# Dense\n",
    "dense_similarity_retriever = faiss_store.as_retriever(\n",
    "    search_type=\"similarity\", search_kwargs={\"k\": K_RE}\n",
    ")\n",
    "\n",
    "ensemble_score = {}\n",
    "\n",
    "for bm25, sim in ((0.3, 0.7), (0.5, 0.5), (0.7, 0.3)):\n",
    "    \n",
    "    ensemble_retriever = EnsembleRetriever(\n",
    "        retrievers=[sparse_bm25_retriever, dense_similarity_retriever],\n",
    "        weights=[bm25, sim],\n",
    "    )\n",
    "\n",
    "    evaluate_retr(\n",
    "        {f\"{bm25}, {sim}\": ensemble_retriever}, # 가중치별\n",
    "        ensemble_score,\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e1d9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "methods = [\"0.3, 0.7\", \"0.5, 0.5\", \"0.7, 0.3\"]\n",
    "\n",
    "\n",
    "\n",
    "precision = [np.nanmean(ensemble_score[weight]['context_precision']) for weight in methods]\n",
    "recall = [np.nanmean(ensemble_score[weight]['context_recall']) for weight in methods]\n",
    "\n",
    "# 시각화\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Precision과 Recall을 나란히 보여주기 위해 bar width 설정\n",
    "bar_width = 0.35\n",
    "index = range(len(methods))\n",
    "\n",
    "# Precision과 Recall Bar 생성\n",
    "bar1 = ax.bar(index, precision, bar_width, label='Precision')\n",
    "bar2 = ax.bar([i + bar_width for i in index], recall, bar_width, label='Recall')\n",
    "\n",
    "# Label 및 제목 설정\n",
    "ax.set_xlabel('weights')\n",
    "ax.set_ylabel('Scores')\n",
    "ax.set_title('Precision and Recall for Different weights')\n",
    "ax.set_xticks([i + bar_width / 2 for i in index])\n",
    "ax.set_xticklabels(methods)\n",
    "ax.legend()\n",
    "\n",
    "# 그래프 출력\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "pprint(ensemble_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71cf8361",
   "metadata": {},
   "source": [
    "# 2. reranking test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0424e5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import CrossEncoderReranker\n",
    "from langchain_community.cross_encoders import HuggingFaceCrossEncoder\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "from langchain.retrievers import EnsembleRetriever\n",
    "import time\n",
    "\n",
    "# 변수\n",
    "K_RE = 10\n",
    "TOP_N = 3\n",
    "# bm25\n",
    "sparse_bm25_retriever = BM25Retriever.from_texts(texts=all_texts)\n",
    "sparse_bm25_retriever.k = K_RE  # k값을 통일하여 설정\n",
    "# Dense\n",
    "dense_similarity_retriever = faiss_store.as_retriever(\n",
    "    search_type=\"similarity\", search_kwargs={\"k\": K_RE}\n",
    ")\n",
    "# ensemble\n",
    "ensemble_retriever = EnsembleRetriever(\n",
    "    retrievers=[sparse_bm25_retriever, dense_similarity_retriever],\n",
    "    weights=[0.3, 0.7],\n",
    ")\n",
    "# ReRanker: CrossEncoder\n",
    "model = HuggingFaceCrossEncoder(model_name=\"BAAI/bge-reranker-base\")\n",
    "compressor = CrossEncoderReranker(model=model, top_n=TOP_N)\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor, base_retriever=ensemble_retriever\n",
    ")\n",
    "\n",
    "final_score = {}\n",
    "time_score = {}\n",
    "\n",
    "start_time = time.time()\n",
    "evaluate_retr(\n",
    "    {f\"rerank\": compression_retriever},\n",
    "    final_score,\n",
    "    # test=True\n",
    ")\n",
    "end_time = time.time()\n",
    "\n",
    "time_score[\"rerank\"] = end_time - start_time\n",
    "\n",
    "# bm25\n",
    "sparse_bm25_retriever = BM25Retriever.from_texts(texts=all_texts)\n",
    "sparse_bm25_retriever.k = 3  # k값을 통일하여 설정\n",
    "# Dense\n",
    "dense_similarity_retriever = faiss_store.as_retriever(\n",
    "    search_type=\"similarity\", search_kwargs={\"k\": 3}\n",
    ")\n",
    "# ensemble\n",
    "ensemble_retriever = EnsembleRetriever(\n",
    "    retrievers=[sparse_bm25_retriever, dense_similarity_retriever],\n",
    "    weights=[0.3, 0.7],\n",
    ")\n",
    "\n",
    "start_time = time.time()\n",
    "evaluate_retr(\n",
    "    {f\"ensemble\": ensemble_retriever},\n",
    "    final_score,\n",
    "    # test=True\n",
    ")\n",
    "end_time = time.time()\n",
    "\n",
    "time_score[\"ensemble\"] = end_time - start_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306856e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pprint(final_score)\n",
    "pprint(time_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
