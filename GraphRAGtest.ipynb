{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce3abde4",
   "metadata": {},
   "source": [
    "# LangGraph test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c9f7e7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langgraph in c:\\users\\ssafy\\desktop\\embedding\\.venv\\lib\\site-packages (0.6.3)\n",
      "Requirement already satisfied: langchain-core>=0.1 in c:\\users\\ssafy\\desktop\\embedding\\.venv\\lib\\site-packages (from langgraph) (0.3.72)\n",
      "Requirement already satisfied: langgraph-checkpoint<3.0.0,>=2.1.0 in c:\\users\\ssafy\\desktop\\embedding\\.venv\\lib\\site-packages (from langgraph) (2.1.1)\n",
      "Requirement already satisfied: langgraph-prebuilt<0.7.0,>=0.6.0 in c:\\users\\ssafy\\desktop\\embedding\\.venv\\lib\\site-packages (from langgraph) (0.6.3)\n",
      "Requirement already satisfied: langgraph-sdk<0.3.0,>=0.2.0 in c:\\users\\ssafy\\desktop\\embedding\\.venv\\lib\\site-packages (from langgraph) (0.2.0)\n",
      "Requirement already satisfied: pydantic>=2.7.4 in c:\\users\\ssafy\\desktop\\embedding\\.venv\\lib\\site-packages (from langgraph) (2.11.7)\n",
      "Requirement already satisfied: xxhash>=3.5.0 in c:\\users\\ssafy\\desktop\\embedding\\.venv\\lib\\site-packages (from langgraph) (3.5.0)\n",
      "Requirement already satisfied: ormsgpack>=1.10.0 in c:\\users\\ssafy\\desktop\\embedding\\.venv\\lib\\site-packages (from langgraph-checkpoint<3.0.0,>=2.1.0->langgraph) (1.10.0)\n",
      "Requirement already satisfied: httpx>=0.25.2 in c:\\users\\ssafy\\desktop\\embedding\\.venv\\lib\\site-packages (from langgraph-sdk<0.3.0,>=0.2.0->langgraph) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.10.1 in c:\\users\\ssafy\\desktop\\embedding\\.venv\\lib\\site-packages (from langgraph-sdk<0.3.0,>=0.2.0->langgraph) (3.11.1)\n",
      "Requirement already satisfied: anyio in c:\\users\\ssafy\\desktop\\embedding\\.venv\\lib\\site-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.0->langgraph) (4.9.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\ssafy\\desktop\\embedding\\.venv\\lib\\site-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.0->langgraph) (2025.7.14)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\ssafy\\desktop\\embedding\\.venv\\lib\\site-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.0->langgraph) (1.0.9)\n",
      "Requirement already satisfied: idna in c:\\users\\ssafy\\desktop\\embedding\\.venv\\lib\\site-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.0->langgraph) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\ssafy\\desktop\\embedding\\.venv\\lib\\site-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.0->langgraph) (0.16.0)\n",
      "Requirement already satisfied: langsmith>=0.3.45 in c:\\users\\ssafy\\desktop\\embedding\\.venv\\lib\\site-packages (from langchain-core>=0.1->langgraph) (0.4.8)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\ssafy\\desktop\\embedding\\.venv\\lib\\site-packages (from langchain-core>=0.1->langgraph) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\ssafy\\desktop\\embedding\\.venv\\lib\\site-packages (from langchain-core>=0.1->langgraph) (1.33)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\ssafy\\desktop\\embedding\\.venv\\lib\\site-packages (from langchain-core>=0.1->langgraph) (6.0.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\ssafy\\desktop\\embedding\\.venv\\lib\\site-packages (from langchain-core>=0.1->langgraph) (4.14.1)\n",
      "Requirement already satisfied: packaging>=23.2 in c:\\users\\ssafy\\desktop\\embedding\\.venv\\lib\\site-packages (from langchain-core>=0.1->langgraph) (25.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\ssafy\\desktop\\embedding\\.venv\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core>=0.1->langgraph) (3.0.0)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\ssafy\\desktop\\embedding\\.venv\\lib\\site-packages (from langsmith>=0.3.45->langchain-core>=0.1->langgraph) (2.32.4)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\ssafy\\desktop\\embedding\\.venv\\lib\\site-packages (from langsmith>=0.3.45->langchain-core>=0.1->langgraph) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in c:\\users\\ssafy\\desktop\\embedding\\.venv\\lib\\site-packages (from langsmith>=0.3.45->langchain-core>=0.1->langgraph) (0.23.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\ssafy\\desktop\\embedding\\.venv\\lib\\site-packages (from pydantic>=2.7.4->langgraph) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\ssafy\\desktop\\embedding\\.venv\\lib\\site-packages (from pydantic>=2.7.4->langgraph) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\ssafy\\desktop\\embedding\\.venv\\lib\\site-packages (from pydantic>=2.7.4->langgraph) (0.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\ssafy\\desktop\\embedding\\.venv\\lib\\site-packages (from requests<3,>=2->langsmith>=0.3.45->langchain-core>=0.1->langgraph) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ssafy\\desktop\\embedding\\.venv\\lib\\site-packages (from requests<3,>=2->langsmith>=0.3.45->langchain-core>=0.1->langgraph) (2.5.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\ssafy\\desktop\\embedding\\.venv\\lib\\site-packages (from anyio->httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.0->langgraph) (1.3.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install langgraph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57088542",
   "metadata": {},
   "source": [
    "# set env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b34b6d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "import os\n",
    "load_dotenv()\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-small\",\n",
    ")\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ[\"OPENAI_API_BASE\"] = os.getenv(\"OPENAI_API_BASE\")\n",
    "\n",
    "faiss_store = FAISS.load_local(\"faiss_store\", embeddings=embeddings, allow_dangerous_deserialization=True) if os.path.exists(\"faiss_store\") else None\n",
    "all_texts = [doc.page_content for doc in faiss_store.docstore._dict.values()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3574d6",
   "metadata": {},
   "source": [
    "# FAISS + ensemble[bm25 / similarity]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d1e576c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.retrievers import BM25Retriever\n",
    "from langchain.retrievers import EnsembleRetriever\n",
    "\n",
    "K = 3\n",
    "weights = [0.3, 0.7]\n",
    "\n",
    "sparse_bm25_retriever = BM25Retriever.from_texts(texts=all_texts)\n",
    "sparse_bm25_retriever.k = K  # k값을 통일하여 설정\n",
    "# Dense\n",
    "dense_similarity_retriever = faiss_store.as_retriever(\n",
    "    search_type=\"similarity\", search_kwargs={\"k\": K}\n",
    ")\n",
    "\n",
    "retriever = EnsembleRetriever(\n",
    "    retrievers=[sparse_bm25_retriever, dense_similarity_retriever],\n",
    "    weights=weights,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43dd2dfd",
   "metadata": {},
   "source": [
    "# state 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a00c52f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, TypedDict\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Graph RAG 파이프라인의 상태를 나타냅니다.\n",
    "\n",
    "    Attributes:\n",
    "        original_question (str): 사용자가 입력한 원본 질문 (그림 묘사)\n",
    "        decomposed_questions (List[str]): 의미 단위로 분해된 질문 리스트\n",
    "        retrieved_contexts (List[str]): 검색된 관련 문서(해석) 조각 리스트\n",
    "        generation (str): LLM이 생성한 최종 해석\n",
    "        relevance_check (str): 질문의 HTP 검사 관련성 여부 (\"yes\" or \"no\")\n",
    "        hallucination_check (str): 생성된 답변의 환각 현상 유무 (\"yes\" or \"no\")\n",
    "    \"\"\"\n",
    "    original_question: str\n",
    "    decomposed_questions: List[str]\n",
    "    retrieved_contexts: List[str]\n",
    "    generation: str\n",
    "    relevance_check: str\n",
    "    hallucination_check: str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cad33d1",
   "metadata": {},
   "source": [
    "# 노드 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9091b8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser, JsonOutputParser\n",
    "\n",
    "# 2-1. Relevance Check Node (질문 관련성 검사)\n",
    "def relevance_check_node(state: GraphState):\n",
    "    \"\"\"\n",
    "    입력된 질문이 HTP 심리검사 해석과 관련이 있는지 확인합니다.\n",
    "    \"\"\"\n",
    "    print(\"--- 1. 질문 관련성 검사 시작 ---\")\n",
    "    question = state[\"original_question\"]\n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_template(\n",
    "        \"\"\"당신은 심리검사 전문가입니다. 주어진 질문이 'HTP(집-나무-사람) 그림 심리검사' 해석과 관련된 내용인지 판단해주세요.\n",
    "        HTP 검사는 집, 나무, 사람 그림의 특징(예: 지붕, 문, 창문, 나무 기둥, 가지, 사람의 눈, 코, 입 등)을 분석하는 것입니다.\n",
    "        이에 해당하면 'yes', 전혀 관련 없는 내용(예: 오늘 날씨, 스포츠 경기 결과 등)이면 'no'로만 대답해주세요.\n",
    "\n",
    "        질문: {question}\n",
    "        판단 (yes/no):\"\"\"\n",
    "    )\n",
    "    \n",
    "    chain = prompt | llm | StrOutputParser()\n",
    "    relevance = chain.invoke({\"question\": question})\n",
    "    \n",
    "    print(f\"질문 관련성: {relevance}\")\n",
    "    state[\"relevance_check\"] = relevance.lower()\n",
    "    return state\n",
    "\n",
    "def decompose_query_node(state: GraphState):\n",
    "    \"\"\"\n",
    "    입력된 질문을 의미 단위의 여러 하위 질문으로 분해합니다.\n",
    "    \"\"\"\n",
    "    print(\"--- 2. 질문 분해 시작 ---\")\n",
    "    question = state[\"original_question\"]\n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_template(\n",
    "        \"\"\"당신은 HTP 그림 심리검사 문장 분석 전문가입니다. 상담사가 그림을 보고 관찰한 내용을 나열한 문장이 주어집니다. \n",
    "        이 문장을 그림의 각 요소(예: 문, 창문, 지붕, 길 등)에 대한 독립적인 해석이 가능한 단위로 분해하여 JSON 리스트 형태로 반환해주세요.\n",
    "\n",
    "        예시:\n",
    "        입력: \"집에 창문은 2개 존재하고 크기는 적절함. 문은 집의 크기에 비해 작으며 문과 바깥이 길로 이어져 있지 않음.\"\n",
    "        출력: {{\"queries\": [\"집 창문의 개수는 2개이고 크기는 적절하다.\", \"집 문의 크기가 집 전체에 비해 작다.\", \"집과 외부를 잇는 길이 그려져 있지 않다.\"]}}\n",
    "\n",
    "        입력: {question}\n",
    "        출력:\"\"\"\n",
    "    )\n",
    "    \n",
    "    chain = prompt | llm | JsonOutputParser()\n",
    "    decomposed = chain.invoke({\"question\": question})\n",
    "    \n",
    "    decomposed_questions = decomposed.get(\"queries\", [])\n",
    "    print(f\"분해된 질문: {decomposed_questions}\")\n",
    "    state[\"decomposed_questions\"] = decomposed_questions\n",
    "    return state\n",
    "\n",
    "# 2-3. Retrieve Node (정보 검색)\n",
    "def retrieve_node(state: GraphState):\n",
    "    \"\"\"\n",
    "    분해된 각 질문에 대해 Ensemble Retriever를 사용하여 관련 문서를 검색합니다.\n",
    "    \"\"\"\n",
    "    print(\"--- 3. 정보 검색 시작 ---\")\n",
    "    decomposed_questions = state[\"decomposed_questions\"]\n",
    "    all_retrieved_docs = []\n",
    "\n",
    "    for query in decomposed_questions:\n",
    "        print(f\"  - 검색 쿼리: '{query}'\")\n",
    "        # 여기서 사용자가 제공한 retriever를 사용합니다.\n",
    "        retrieved_docs = retriever.invoke(query)\n",
    "        \n",
    "        # 검색 결과의 내용을 문자열로 변환하여 추가\n",
    "        doc_texts = [doc.page_content for doc in retrieved_docs]\n",
    "        all_retrieved_docs.extend(doc_texts)\n",
    "    \n",
    "    # 중복 제거\n",
    "    unique_contexts = list(set(all_retrieved_docs))\n",
    "    print(f\"검색된 해석 Context 수: {len(unique_contexts)}\")\n",
    "    state[\"retrieved_contexts\"] = unique_contexts\n",
    "    return state\n",
    "\n",
    "# 2-4. Generate Node (답변 생성)\n",
    "def generate_node(state: GraphState):\n",
    "    \"\"\"\n",
    "    검색된 Context를 바탕으로 최종 해석 답변을 생성합니다.\n",
    "    \"\"\"\n",
    "    print(\"--- 4. 답변 생성 시작 ---\")\n",
    "    question = state[\"original_question\"]\n",
    "    contexts = \"\\n\\n\".join(state[\"retrieved_contexts\"])\n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_template(\n",
    "        \"\"\"당신은 HTP 그림 심리검사 결과 해석 전문가입니다.\n",
    "        상담사가 관찰한 '그림 특징'과 그에 대한 '해석 참고자료'가 주어집니다. \n",
    "        두 정보를 종합하여, 내담자의 심리상태에 대한 최종 해석 보고서를 작성해주세요.\n",
    "        전문적이고 이해하기 쉬운 말투로 설명하고, 각 특징과 해석을 논리적으로 연결하여 설명해주세요.\n",
    "\n",
    "        ## 상담사의 그림 특징 관찰 내용:\n",
    "        {question}\n",
    "\n",
    "        ## 해석 참고자료:\n",
    "        {context}\n",
    "\n",
    "        ## 최종 해석 보고서:\n",
    "        \"\"\"\n",
    "    )\n",
    "    \n",
    "    chain = prompt | llm | StrOutputParser()\n",
    "    generation = chain.invoke({\"question\": question, \"context\": contexts})\n",
    "    \n",
    "    print(\"생성된 답변 일부:\", generation[:200] + \"...\")\n",
    "    state[\"generation\"] = generation\n",
    "    return state\n",
    "\n",
    "# 2-5. Hallucination Check Node (환각 검사)\n",
    "def hallucination_check_node(state: GraphState):\n",
    "    \"\"\"\n",
    "    생성된 답변에 환각(hallucination)이 있는지 검사합니다.\n",
    "    \"\"\"\n",
    "    print(\"--- 5. 환각 검사 시작 ---\")\n",
    "    contexts = state[\"retrieved_contexts\"]\n",
    "    generation = state[\"generation\"]\n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_template(\n",
    "        \"\"\"당신은 AI 답변 검증 전문가입니다. 주어진 '참고 자료'를 바탕으로 '생성된 답변'이 만들어졌는지 확인해야 합니다.\n",
    "        '생성된 답변'의 모든 내용이 '참고 자료'에 근거하고 있다면 'yes'를, '참고 자료'에 없는 내용이 포함되어 있다면 'no'를 반환해주세요.\n",
    "\n",
    "        ## 참고 자료:\n",
    "        {context}\n",
    "\n",
    "        ## 생성된 답변:\n",
    "        {generation}\n",
    "\n",
    "        판단 (yes/no):\"\"\"\n",
    "    )\n",
    "    \n",
    "    chain = prompt | llm | StrOutputParser()\n",
    "    check_result = chain.invoke({\"context\": \"\\n\\n\".join(contexts), \"generation\": generation})\n",
    "\n",
    "    print(f\"환각 검사 결과: {check_result}\")\n",
    "    state[\"hallucination_check\"] = check_result.lower()\n",
    "    return state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af20e5c4",
   "metadata": {},
   "source": [
    "# Edge 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "30bccccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3-1. 질문 관련성 검사 후 분기\n",
    "def decide_after_relevance_check(state: GraphState):\n",
    "    \"\"\"\n",
    "    질문 관련성 검사 결과에 따라 다음 단계를 결정합니다.\n",
    "    - \"yes\": 질문 분해 단계로 이동\n",
    "    - \"no\": 종료\n",
    "    \"\"\"\n",
    "    if state[\"relevance_check\"] == \"yes\":\n",
    "        print(\"결과: 관련성 있음. 질문 분해를 진행합니다.\")\n",
    "        return \"decompose\"\n",
    "    else:\n",
    "        print(\"결과: 관련성 없음. 프로세스를 종료합니다.\")\n",
    "        return \"end\"\n",
    "\n",
    "# 3-2. 환각 검사 후 분기\n",
    "def decide_after_hallucination_check(state: GraphState):\n",
    "    \"\"\"\n",
    "    환각 검사 결과에 따라 다음 단계를 결정합니다.\n",
    "    - \"yes\": 환각 없음, 종료\n",
    "    - \"no\": 환각 존재, 답변 재생성\n",
    "    \"\"\"\n",
    "    if state[\"hallucination_check\"] == \"yes\":\n",
    "        print(\"결과: 환각 없음. 최종 답변을 반환합니다.\")\n",
    "        return \"end\"\n",
    "    else:\n",
    "        print(\"결과: 환각 존재. 답변을 다시 생성합니다.\")\n",
    "        return \"regenerate\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5d1668",
   "metadata": {},
   "source": [
    "# 그래프 통합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de444f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END, StateGraph\n",
    "\n",
    "# 그래프 빌더 생성\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# 노드 추가\n",
    "workflow.add_node(\"relevance_check\", relevance_check_node)\n",
    "workflow.add_node(\"decompose_query\", decompose_query_node)\n",
    "workflow.add_node(\"retrieve\", retrieve_node)\n",
    "workflow.add_node(\"generate\", generate_node)\n",
    "workflow.add_node(\"hallucination_check\", hallucination_check_node)\n",
    "\n",
    "# 엣지 연결\n",
    "workflow.set_entry_point(\"relevance_check\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"relevance_check\",\n",
    "    decide_after_relevance_check,\n",
    "    {\"decompose\": \"decompose_query\", \"end\": END}\n",
    ")\n",
    "workflow.add_edge(\"decompose_query\", \"retrieve\")\n",
    "workflow.add_edge(\"retrieve\", \"generate\")\n",
    "workflow.add_edge(\"generate\", \"hallucination_check\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"hallucination_check\",\n",
    "    decide_after_hallucination_check,\n",
    "    {\"regenerate\": \"generate\", \"end\": END}\n",
    ")\n",
    "\n",
    "# 그래프 컴파일\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4069d161",
   "metadata": {},
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b07e3708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1. 질문 관련성 검사 시작 ---\n",
      "질문 관련성: yes\n",
      "결과: 관련성 있음. 질문 분해를 진행합니다.\n",
      "--- 2. 질문 분해 시작 ---\n",
      "분해된 질문: ['집 창문의 개수는 2개이고 크기는 적절하다.', '집 문의 크기가 집 전체에 비해 작다.', '집과 외부를 잇는 길이 그려져 있지 않다.']\n",
      "--- 3. 정보 검색 시작 ---\n",
      "  - 검색 쿼리: '집 창문의 개수는 2개이고 크기는 적절하다.'\n",
      "  - 검색 쿼리: '집 문의 크기가 집 전체에 비해 작다.'\n",
      "  - 검색 쿼리: '집과 외부를 잇는 길이 그려져 있지 않다.'\n",
      "검색된 해석 Context 수: 12\n",
      "--- 4. 답변 생성 시작 ---\n",
      "생성된 답변 일부: ## 최종 해석 보고서\n",
      "\n",
      "### 개요\n",
      "본 보고서는 HTP(집-나무-사람) 그림 심리검사를 통해 내담자의 심리적 상태를 분석하고 해석한 내용입니다. 그림의 세부적 요소를 바탕으로 내담자가 가족과의 관계, 대인관계, 그리고 외부 세계에 대한 태도 및 감정을 추론합니다.\n",
      "\n",
      "### 집의 창문과 문\n",
      "내담자의 그림에서는 집에 창문이 2개 그려져 있으며, 크기는 적절합...\n",
      "--- 5. 환각 검사 시작 ---\n",
      "환각 검사 결과: yes\n",
      "결과: 환각 없음. 최종 답변을 반환합니다.\n"
     ]
    },
    {
     "ename": "GraphRecursionError",
     "evalue": "Recursion limit of 5 reached without hitting a stop condition. You can increase the limit by setting the `recursion_limit` config key.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/GRAPH_RECURSION_LIMIT",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mGraphRecursionError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m      2\u001b[39m inputs = {\n\u001b[32m      3\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33moriginal_question\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33m집에 창문은 2개 존재하고 크기는 적절함. 문은 집의 크기에 비해 작으며 문과 바깥이 길로 이어져 있지 않음.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      4\u001b[39m }\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# # 그래프 실행\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# for output in app.stream(inputs, {\"recursion_limit\": 5}): # 순환 방지를 위해 recursion_limit 설정\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m#     for key, value in output.items():\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     13\u001b[39m \n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# 최종 결과 확인\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m final_state = \u001b[43mapp\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrecursion_limit\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m + \u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m50\u001b[39m)\n\u001b[32m     17\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m          최종 HTP 심리검사 해석 결과\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\SSAFY\\Desktop\\Embedding\\.venv\\Lib\\site-packages\\langgraph\\pregel\\main.py:3015\u001b[39m, in \u001b[36mPregel.invoke\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, **kwargs)\u001b[39m\n\u001b[32m   3012\u001b[39m chunks: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] | Any] = []\n\u001b[32m   3013\u001b[39m interrupts: \u001b[38;5;28mlist\u001b[39m[Interrupt] = []\n\u001b[32m-> \u001b[39m\u001b[32m3015\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3016\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   3017\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3018\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3019\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mupdates\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m   3020\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m   3021\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3022\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3023\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3024\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3025\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3026\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdurability\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdurability\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3027\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3028\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   3029\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\n\u001b[32m   3030\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\SSAFY\\Desktop\\Embedding\\.venv\\Lib\\site-packages\\langgraph\\pregel\\main.py:2670\u001b[39m, in \u001b[36mPregel.stream\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, subgraphs, debug, **kwargs)\u001b[39m\n\u001b[32m   2661\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m loop.status == \u001b[33m\"\u001b[39m\u001b[33mout_of_steps\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   2662\u001b[39m     msg = create_error_message(\n\u001b[32m   2663\u001b[39m         message=(\n\u001b[32m   2664\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRecursion limit of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig[\u001b[33m'\u001b[39m\u001b[33mrecursion_limit\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m reached \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m   2668\u001b[39m         error_code=ErrorCode.GRAPH_RECURSION_LIMIT,\n\u001b[32m   2669\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m2670\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m GraphRecursionError(msg)\n\u001b[32m   2671\u001b[39m \u001b[38;5;66;03m# set final channel values as run output\u001b[39;00m\n\u001b[32m   2672\u001b[39m run_manager.on_chain_end(loop.output)\n",
      "\u001b[31mGraphRecursionError\u001b[39m: Recursion limit of 5 reached without hitting a stop condition. You can increase the limit by setting the `recursion_limit` config key.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/GRAPH_RECURSION_LIMIT"
     ]
    }
   ],
   "source": [
    "# HTP 그림 검사 예시 질문\n",
    "inputs = {\n",
    "    \"original_question\": \"집에 창문은 2개 존재하고 크기는 적절함. 문은 집의 크기에 비해 작으며 문과 바깥이 길로 이어져 있지 않음.\"\n",
    "}\n",
    "\n",
    "# # 그래프 실행\n",
    "# for output in app.stream(inputs, {\"recursion_limit\": 5}): # 순환 방지를 위해 recursion_limit 설정\n",
    "#     for key, value in output.items():\n",
    "#         # 각 단계의 최종 출력만 표시\n",
    "#         print(f\"노드 '{key}' 완료:\")\n",
    "#         # print(f\"  - 상태: {value}\") # 전체 상태를 보려면 주석 해제\n",
    "#         print(\"---\")\n",
    "\n",
    "# 최종 결과 확인\n",
    "final_state = app.invoke(inputs, {\"recursion_limit\": 5})\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"          최종 HTP 심리검사 해석 결과\")\n",
    "print(\"=\"*50)\n",
    "print(final_state['generation'])\n",
    "print(\"=\"*50)\n",
    "\n",
    "# 관련 없는 질문 예시\n",
    "print(\"\\n\\n\" + \"=\"*50)\n",
    "print(\"          관련 없는 질문 테스트\")\n",
    "print(\"=\"*50)\n",
    "inputs_irrelevant = {\"original_question\": \"오늘 프로야구 경기 결과는 어떻게 되나요?\"}\n",
    "irrelevant_result = app.invoke(inputs_irrelevant)\n",
    "print(\"최종 상태:\", irrelevant_result)\n",
    "print(\"=\"*50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
