{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce3abde4",
   "metadata": {},
   "source": [
    "# LangGraph test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9f7e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langgraph dotenv langchain langchain-openai langchain-community openai faiss-cpu rank_bm25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57088542",
   "metadata": {},
   "source": [
    "# set env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b34b6d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ[\"OPENAI_API_BASE\"] = os.getenv(\"OPENAI_API_BASE\")\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "faiss_store = (\n",
    "    FAISS.load_local(\n",
    "        \"faiss_store\", embeddings=embeddings, allow_dangerous_deserialization=True\n",
    "    )\n",
    "    if os.path.exists(\"faiss_store\")\n",
    "    else None\n",
    ")\n",
    "\n",
    "all_docs = list(faiss_store.docstore._dict.values())\n",
    "\n",
    "\n",
    "all_texts = [doc.page_content for doc in faiss_store.docstore._dict.values()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3574d6",
   "metadata": {},
   "source": [
    "# FAISS + ensemble[bm25 / similarity]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "7d1e576c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ BM25 캐시 생성 완료! 대상: ['집', '나무', '사람', '공통 구조 해석']\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.retrievers import BM25Retriever\n",
    "from langchain.retrievers import EnsembleRetriever\n",
    "\n",
    "common_docs = [doc for doc in all_docs if doc.metadata.get(\"main_topic\") == \"공통 구조 해석\"]\n",
    "\n",
    "# 나머지 카테고리별로 문서 그룹화\n",
    "specific_docs_grouped = defaultdict(list)\n",
    "for doc in all_docs:\n",
    "    category = doc.metadata.get(\"main_topic\")\n",
    "    if category and category != \"공통 구조 해석\":\n",
    "        specific_docs_grouped[category].append(doc)\n",
    "\n",
    "# BM25 리트리버 캐시 생성: 각 카테고리에 '공통' 문서를 합쳐서 생성\n",
    "bm25_retriever_cache = {}\n",
    "for category, docs in specific_docs_grouped.items():\n",
    "    # 해당 카테고리 문서와 공통 문서를 합칩니다.\n",
    "    combined_docs = docs + common_docs\n",
    "    bm25_retriever_cache[category] = BM25Retriever.from_documents(combined_docs)\n",
    "\n",
    "# '공통 해석 사항' 자체에 대한 리트리버도 추가 (필요 시 사용)\n",
    "if common_docs:\n",
    "    bm25_retriever_cache[\"공통 구조 해석\"] = BM25Retriever.from_documents(common_docs)\n",
    "\n",
    "print(f\"✅ BM25 캐시 생성 완료! 대상: {list(bm25_retriever_cache.keys())}\")\n",
    "\n",
    "def get_ensemble_retriever_by_category(\n",
    "    faiss_store: FAISS,\n",
    "    bm25_cache: dict,\n",
    "    category: str,\n",
    "    k: int = 3,\n",
    "    weights: list[float] = [0.3, 0.7]\n",
    "):\n",
    "    \"\"\"\n",
    "    지정된 카테고리(+공통)에 맞는 리트리버를 동적으로 조합하여 반환합니다.\n",
    "    \"\"\"\n",
    "    # 1. 캐시에서 미리 생성된 BM25 리트리버를 가져옵니다.\n",
    "    # 이 리트리버는 이미 '공통' 문서를 포함하고 있습니다.\n",
    "    sparse_bm25_retriever = bm25_cache.get(category)\n",
    "    if not sparse_bm25_retriever:\n",
    "        raise ValueError(f\"'{category}' 카테고리에 대한 BM25 리트리버가 캐시에 없습니다.\")\n",
    "    sparse_bm25_retriever.k = k\n",
    "\n",
    "    # 2. FAISS 리트리버는 'filter'와 '$in'을 사용하여 동적으로 생성합니다.\n",
    "    # '공통'과 '지정된 카테고리'를 모두 포함하도록 필터링합니다.\n",
    "    search_categories = [\"공통 구조 해석\", category]\n",
    "    # 만약 카테고리가 '공통 해석 사항'이라면 중복을 피합니다.\n",
    "    if category == \"공통 구조 해석\":\n",
    "        search_categories = [\"공통 구조 해석\"]\n",
    "    \n",
    "    dense_similarity_retriever = faiss_store.as_retriever(\n",
    "        search_type=\"similarity\",\n",
    "        search_kwargs={\n",
    "            \"k\": k, \n",
    "            # 중요: 메타데이터 키를 'main_topic'으로 통일합니다.\n",
    "            \"filter\": {\"main_topic\": {\"$in\": search_categories}}\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # 3. 두 리트리버를 앙상블로 묶습니다.\n",
    "    return EnsembleRetriever(\n",
    "        retrievers=[sparse_bm25_retriever, dense_similarity_retriever],\n",
    "        weights=weights,\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "K = 3\n",
    "weights = [0.3, 0.7]\n",
    "\n",
    "# Sparse\n",
    "sparse_bm25_retriever = BM25Retriever.from_texts(texts=all_texts)\n",
    "sparse_bm25_retriever.k = K  # k값을 통일하여 설정\n",
    "# Dense\n",
    "dense_similarity_retriever = faiss_store.as_retriever(\n",
    "    search_type=\"similarity\", search_kwargs={\"k\": K}\n",
    ")\n",
    "\n",
    "retriever = EnsembleRetriever(\n",
    "    retrievers=[sparse_bm25_retriever, dense_similarity_retriever],\n",
    "    weights=weights,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43dd2dfd",
   "metadata": {},
   "source": [
    "# state 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "a00c52f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, TypedDict\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Graph RAG 파이프라인의 상태\n",
    "\n",
    "    Attributes:\n",
    "        original_question (str): 사용자가 입력한 원본 질문 (그림 묘사)\n",
    "        decomposed_questions (List[str]): 의미 단위로 분해된 질문 리스트\n",
    "        retrieved_contexts (List[str]): 검색된 관련 문서(해석) 조각 리스트\n",
    "        generation (str): LLM이 생성한 최종 해석\n",
    "        relevance_check (str): 질문의 HTP 검사 관련성 여부 (\"yes\" or \"no\")\n",
    "        hallucination_check (str): 생성된 답변의 환각 현상 유무 (\"yes\" or \"no\")\n",
    "        category (str): 질문 범주(집, 나무, 사람)\n",
    "    \"\"\"\n",
    "    original_question: str\n",
    "    decomposed_questions: List[str]\n",
    "    retrieved_contexts: List[str]\n",
    "    generation: str\n",
    "    relevance_check: str\n",
    "    hallucination_check: str\n",
    "    category: str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b794cc",
   "metadata": {},
   "source": [
    "# 실행 시간 체크"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "bade9340",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import functools\n",
    "\n",
    "execution_times = {}\n",
    "\n",
    "def timing_decorator(func):\n",
    "    \"\"\"\n",
    "    각 노드 실행 시간 측정\n",
    "    \"\"\"\n",
    "    @functools.wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start_time = time.time()\n",
    "        result = func(*args, **kwargs)\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "        print(f\"Execution time for {func.__name__}: {elapsed_time:.2f} seconds\")\n",
    "        execution_times[func.__name__] = elapsed_time\n",
    "        return result\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cad33d1",
   "metadata": {},
   "source": [
    "# 노드 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "9091b8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser, JsonOutputParser\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "\n",
    "# 2-1. Relevance Check Node (질문 관련성 검사)\n",
    "@timing_decorator\n",
    "def relevance_check_node(state: GraphState):\n",
    "    \"\"\"\n",
    "    입력된 질문이 HTP 심리검사 해석과 관련이 있는지 확인합니다.\n",
    "    \"\"\"\n",
    "    print(\"--- 1. 질문 관련성 검사 시작 ---\")\n",
    "    question = state[\"original_question\"]\n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_template(\n",
    "        \"\"\"당신은 심리검사 전문가입니다. 주어진 질문이 'HTP(집-나무-사람) 그림 심리검사' 해석과 관련된 내용인지 판단해주세요.\n",
    "        HTP 검사는 집, 나무, 사람 그림의 특징(예: 지붕, 문, 창문, 나무 기둥, 가지, 사람의 눈, 코, 입 등)을 분석하는 것입니다.\n",
    "        질문은 HTP 그림에 대한 관찰 묘사로, 그림의 요소나 특징에 대한 내용입니다.\n",
    "        이에 해당하면 'yes', 전혀 관련 없는 내용(예: 오늘 날씨, 스포츠 경기 결과 등)이면 'no'로만 대답해주세요.\n",
    "\n",
    "        질문: {question}\n",
    "        판단 (yes/no):\"\"\"\n",
    "    )\n",
    "    \n",
    "    chain = prompt | llm | StrOutputParser()\n",
    "    relevance = chain.invoke({\"question\": question})\n",
    "    \n",
    "    print(f\"질문 관련성: {relevance}\")\n",
    "    state[\"relevance_check\"] = relevance.lower()\n",
    "    if state[\"relevance_check\"] == \"no\":\n",
    "        # print(\"질문이 HTP 검사와 관련이 없습니다. 프로세스를 종료합니다.\")\n",
    "        state[\"generation\"] = \"관찰 결과를 다시 입력해주세요.\"\n",
    "        print(state)\n",
    "    return state\n",
    "\n",
    "# 2-2. Decompose Node (질문 분해)\n",
    "@timing_decorator\n",
    "def decompose_query_node(state: GraphState):\n",
    "    \"\"\"\n",
    "    입력된 질문을 의미 단위의 여러 하위 질문으로 분해합니다.\n",
    "    \"\"\"\n",
    "    print(\"--- 2. 질문 분해 시작 ---\")\n",
    "    question = state[\"original_question\"]\n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_template(\n",
    "        \"\"\"당신은 HTP 그림 심리검사 문장 분석 전문가입니다. 상담사가 그림을 보고 관찰한 내용을 나열한 문장이 주어집니다. \n",
    "        이 문장을 그림의 각 요소(예: 문, 창문, 지붕, 길 등)에 대한 독립적인 해석이 가능한 단위로 분해하여 JSON 리스트 형태로 반환해주세요.\n",
    "\n",
    "        예시:\n",
    "        입력: \"집에 창문은 2개 존재하고 크기는 적절함. 문은 집의 크기에 비해 작으며 문과 바깥이 길로 이어져 있지 않음.\"\n",
    "        출력: {{\"queries\": [\"집 창문의 개수는 2개이고 크기는 적절하다.\", \"집 문의 크기가 집 전체에 비해 작다.\", \"집과 외부를 잇는 길이 그려져 있지 않다.\"]}}\n",
    "\n",
    "        입력: {question}\n",
    "        출력:\"\"\"\n",
    "    )\n",
    "    \n",
    "    chain = prompt | llm | JsonOutputParser()\n",
    "    decomposed = chain.invoke({\"question\": question})\n",
    "    \n",
    "    decomposed_questions = decomposed.get(\"queries\", [])\n",
    "    print(f\"분해된 질문: {decomposed_questions}\")\n",
    "    state[\"decomposed_questions\"] = decomposed_questions\n",
    "    return state\n",
    "\n",
    "# 2-3. Retrieve Node (정보 검색)\n",
    "@timing_decorator\n",
    "def meta_retrieve_node(state: GraphState, config: RunnableConfig):\n",
    "    \"\"\"\n",
    "    분해된 각 질문에 대해, state의 category에 맞춰 동적으로 Ensemble Retriever를\n",
    "    생성하고 관련 문서를 검색합니다.\n",
    "    \"\"\"\n",
    "    print(f\"--- 3. 정보 검색 시작 (카테고리: {state['category']}) ---\")\n",
    "    \n",
    "    # FastAPI의 app.state에 저장해 둔 구성요소를 config를 통해 가져옵니다.\n",
    "    faiss_store = config[\"configurable\"][\"faiss_store\"]\n",
    "    bm25_cache = config[\"configurable\"][\"bm25_retriever_cache\"]\n",
    "    \n",
    "    # 현재 요청의 카테고리에 맞는 앙상블 리트리버를 즉시 생성합니다.\n",
    "    try:\n",
    "        retriever = get_ensemble_retriever_by_category(\n",
    "            faiss_store=faiss_store,\n",
    "            bm25_cache=bm25_cache,\n",
    "            category=state[\"category\"]\n",
    "        )\n",
    "    except ValueError as e:\n",
    "        # 특정 카테고리가 없는 경우에 대한 예외 처리\n",
    "        print(e)\n",
    "        state[\"retrieved_contexts\"] = []\n",
    "        # 또는 에러 메시지를 generation에 담아 사용자에게 전달할 수도 있습니다.\n",
    "        state[\"generation\"] = \"죄송합니다. 요청하신 카테고리의 정보를 찾을 수 없습니다.\"\n",
    "        return state\n",
    "\n",
    "    decomposed_questions = state[\"decomposed_questions\"]\n",
    "    all_retrieved_docs = []\n",
    "    for query in decomposed_questions:\n",
    "        print(f\"  - 검색 쿼리: '{query}'\")\n",
    "        retrieved_docs = retriever.invoke(query)\n",
    "        doc_texts = [doc.page_content for doc in retrieved_docs]\n",
    "        all_retrieved_docs.extend(doc_texts)\n",
    "    \n",
    "    unique_contexts = list(set(all_retrieved_docs))\n",
    "    print(f\"검색된 해석 Context 수: {len(unique_contexts)}\")\n",
    "    state[\"retrieved_contexts\"] = unique_contexts\n",
    "    return state\n",
    "\n",
    "@timing_decorator\n",
    "def retrieve_node(state: GraphState):\n",
    "    \"\"\"\n",
    "    분해된 각 질문에 대해 Ensemble Retriever를 사용하여 관련 문서를 검색합니다.\n",
    "    \"\"\"\n",
    "    print(\"--- 3. 정보 검색 시작 ---\")\n",
    "    decomposed_questions = state[\"decomposed_questions\"]\n",
    "    all_retrieved_docs = []\n",
    "\n",
    "    for query in decomposed_questions:\n",
    "        print(f\"  - 검색 쿼리: '{query}'\")\n",
    "        # 여기서 사용자가 제공한 retriever를 사용합니다.\n",
    "        retrieved_docs = retriever.invoke(query)\n",
    "        \n",
    "        # 검색 결과의 내용을 문자열로 변환하여 추가\n",
    "        doc_texts = [doc.page_content for doc in retrieved_docs]\n",
    "        all_retrieved_docs.extend(doc_texts)\n",
    "    \n",
    "    # 중복 제거\n",
    "    unique_contexts = list(set(all_retrieved_docs))\n",
    "    print(f\"검색된 해석 Context 수: {len(unique_contexts)}\")\n",
    "    state[\"retrieved_contexts\"] = unique_contexts\n",
    "    return state\n",
    "\n",
    "# 2-4. Generate Node (답변 생성)\n",
    "@timing_decorator\n",
    "def generate_node(state: GraphState):\n",
    "    \"\"\"\n",
    "    검색된 Context를 바탕으로 최종 해석 답변을 생성합니다.\n",
    "    \"\"\"\n",
    "    print(\"--- 4. 답변 생성 시작 ---\")\n",
    "    question = state[\"original_question\"]\n",
    "    contexts = \"\\n\\n\".join(state[\"retrieved_contexts\"])\n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_template(\n",
    "        \"\"\"당신은 HTP 그림 심리검사 결과 해석 전문가입니다.\n",
    "        상담사가 관찰한 '그림 특징'과 그에 대한 '해석 참고자료'가 주어집니다. \n",
    "        두 정보를 종합하여, 내담자의 심리상태에 대한 최종 해석 보고서를 작성해주세요.\n",
    "        전문적이고 이해하기 쉬운 말투로 설명하고, 각 특징과 해석을 논리적으로 연결하여 설명해주세요.\n",
    "        정보는 반드시 주어진 '해석 참고자료'에 근거해야 하며, 환각(hallucination)이 없어야 합니다.\n",
    "        답변은 질문에 대한 관찰 내용과 관련된 것만 작성해야 합니다.\n",
    "        각 특징에 대한 해석 내용을 분석하고 참고 자료에서 관련 내용을 찾아 종합적으로 해석하세요.\n",
    "        답변은 너무 극단적이지 않고 충분히 납득할 수 있는 수준으로 작성해야 합니다.\n",
    "        참고자료가 너무 적거나 관련성이 낮다면, 답변은 간단하게 작성해주세요.\n",
    "\n",
    "        ## 상담사의 그림 특징 관찰 내용:\n",
    "        {question}\n",
    "\n",
    "        ## 해석 참고자료:\n",
    "        {context}\n",
    "\n",
    "        ## 최종 해석 보고서:\n",
    "        \"\"\"\n",
    "    )\n",
    "    \n",
    "    chain = prompt | llm | StrOutputParser()\n",
    "    generation = chain.invoke({\"question\": question, \"context\": contexts})\n",
    "    \n",
    "    print(\"생성된 답변 일부:\", generation[:200] + \"...\")\n",
    "    state[\"generation\"] = generation\n",
    "    return state\n",
    "\n",
    "# 2-5. Hallucination Check Node (환각 검사)\n",
    "@timing_decorator\n",
    "def hallucination_check_node(state: GraphState):\n",
    "    \"\"\"\n",
    "    생성된 답변에 환각(hallucination)이 있는지 검사합니다.\n",
    "    \"\"\"\n",
    "    print(\"--- 5. 환각 검사 시작 ---\")\n",
    "    contexts = state[\"retrieved_contexts\"]\n",
    "    generation = state[\"generation\"]\n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_template(\n",
    "        \"\"\"당신은 AI 답변 검증 전문가입니다. 주어진 '참고 자료'를 바탕으로 '생성된 답변'이 만들어졌는지 확인해야 합니다.\n",
    "        '생성된 답변'의 모든 내용이 '참고 자료'에 근거하고 있다면 'yes'를, '참고 자료'에 없는 내용이 포함되어 있다면 'no'를 반환해주세요.\n",
    "\n",
    "        ## 참고 자료:\n",
    "        {context}\n",
    "\n",
    "        ## 생성된 답변:\n",
    "        {generation}\n",
    "\n",
    "        판단 (yes/no):\"\"\"\n",
    "    )\n",
    "    \n",
    "    chain = prompt | llm | StrOutputParser()\n",
    "    check_result = chain.invoke({\"context\": \"\\n\\n\".join(contexts), \"generation\": generation})\n",
    "\n",
    "    print(f\"환각 검사 결과: {check_result}\")\n",
    "    state[\"hallucination_check\"] = check_result.lower()\n",
    "    return state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af20e5c4",
   "metadata": {},
   "source": [
    "# Edge 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "30bccccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3-1. 질문 관련성 검사 후 분기\n",
    "def decide_after_relevance_check(state: GraphState):\n",
    "    \"\"\"\n",
    "    질문 관련성 검사 결과에 따라 다음 단계를 결정합니다.\n",
    "    - \"yes\": 질문 분해 단계로 이동\n",
    "    - \"no\": 종료\n",
    "    \"\"\"\n",
    "    if state[\"relevance_check\"] == \"yes\":\n",
    "        print(\"결과: 관련성 있음. 질문 분해를 진행합니다.\")\n",
    "        return \"decompose\"\n",
    "    else:\n",
    "        print(\"결과: 관련성 없음. 프로세스를 종료합니다.\")\n",
    "        return \"end\"\n",
    "\n",
    "# 3-2. 환각 검사 후 분기\n",
    "def decide_after_hallucination_check(state: GraphState):\n",
    "    \"\"\"\n",
    "    환각 검사 결과에 따라 다음 단계를 결정합니다.\n",
    "    - \"yes\": 환각 없음, 종료\n",
    "    - \"no\": 환각 존재, 답변 재생성\n",
    "    \"\"\"\n",
    "    if state[\"hallucination_check\"] == \"yes\":\n",
    "        print(\"결과: 환각 없음. 최종 답변을 반환합니다.\")\n",
    "        # print(state)\n",
    "        return \"end\"\n",
    "    else:\n",
    "        print(\"결과: 환각 존재. 답변을 다시 생성합니다.\")\n",
    "        return \"regenerate\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5d1668",
   "metadata": {},
   "source": [
    "# 그래프 통합\n",
    "\n",
    "## Naive RAG vs Graph RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "de444f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END, StateGraph\n",
    "\n",
    "# 그래프 빌더 생성\n",
    "def create_graph_rag():\n",
    "    graph_workflow = StateGraph(GraphState)\n",
    "\n",
    "    # 노드 추가\n",
    "    graph_workflow.add_node(\"relevance_check\", relevance_check_node)\n",
    "    graph_workflow.add_node(\"decompose_query\", decompose_query_node)\n",
    "    graph_workflow.add_node(\"retrieve\", retrieve_node)\n",
    "    graph_workflow.add_node(\"generate\", generate_node)\n",
    "    graph_workflow.add_node(\"hallucination_check\", hallucination_check_node)\n",
    "\n",
    "    # 엣지 연결\n",
    "    graph_workflow.set_entry_point(\"relevance_check\")\n",
    "    graph_workflow.add_conditional_edges(\n",
    "        \"relevance_check\",\n",
    "        decide_after_relevance_check,\n",
    "        {\"decompose\": \"decompose_query\", \"end\": END}\n",
    "    )\n",
    "    graph_workflow.add_edge(\"decompose_query\", \"retrieve\")\n",
    "    graph_workflow.add_edge(\"retrieve\", \"generate\")\n",
    "    graph_workflow.add_edge(\"generate\", \"hallucination_check\")\n",
    "    graph_workflow.add_conditional_edges(\n",
    "        \"hallucination_check\",\n",
    "        decide_after_hallucination_check,\n",
    "        {\"regenerate\": \"generate\", \"end\": END}\n",
    "    )\n",
    "\n",
    "    # 그래프 컴파일\n",
    "    return graph_workflow.compile()\n",
    "\n",
    "def create_meta_graph_rag():\n",
    "    graph_workflow = StateGraph(GraphState)\n",
    "\n",
    "    # 노드 추가\n",
    "    graph_workflow.add_node(\"relevance_check\", relevance_check_node)\n",
    "    graph_workflow.add_node(\"decompose_query\", decompose_query_node)\n",
    "    graph_workflow.add_node(\"retrieve\", meta_retrieve_node)\n",
    "    graph_workflow.add_node(\"generate\", generate_node)\n",
    "    graph_workflow.add_node(\"hallucination_check\", hallucination_check_node)\n",
    "\n",
    "    # 엣지 연결\n",
    "    graph_workflow.set_entry_point(\"relevance_check\")\n",
    "    graph_workflow.add_conditional_edges(\n",
    "        \"relevance_check\",\n",
    "        decide_after_relevance_check,\n",
    "        {\"decompose\": \"decompose_query\", \"end\": END}\n",
    "    )\n",
    "    graph_workflow.add_edge(\"decompose_query\", \"retrieve\")\n",
    "    graph_workflow.add_edge(\"retrieve\", \"generate\")\n",
    "    graph_workflow.add_edge(\"generate\", \"hallucination_check\")\n",
    "    graph_workflow.add_conditional_edges(\n",
    "        \"hallucination_check\",\n",
    "        decide_after_hallucination_check,\n",
    "        {\"regenerate\": \"generate\", \"end\": END}\n",
    "    )\n",
    "\n",
    "    # 그래프 컴파일\n",
    "    return graph_workflow.compile()\n",
    "\n",
    "def create_naive_rag():\n",
    "    naive_workflow = StateGraph(GraphState)\n",
    "    # 노드 추가\n",
    "    naive_workflow.add_node(\"decompose_query\", decompose_query_node)\n",
    "    naive_workflow.add_node(\"retrieve\", retrieve_node)\n",
    "    naive_workflow.add_node(\"generate\", generate_node)\n",
    "\n",
    "    # 엣지 연결\n",
    "    naive_workflow.set_entry_point(\"decompose_query\")\n",
    "    naive_workflow.add_edge(\"decompose_query\", \"retrieve\")\n",
    "    naive_workflow.add_edge(\"retrieve\", \"generate\")\n",
    "    naive_workflow.add_edge(\"generate\", END)\n",
    "    # 그래프 컴파일\n",
    "    return naive_workflow.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4069d161",
   "metadata": {},
   "source": [
    "# test 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "b07e3708",
   "metadata": {},
   "outputs": [],
   "source": [
    "# app 생성\n",
    "graph_app = create_graph_rag()\n",
    "naive_app = create_naive_rag()\n",
    "graph_meta_app = create_meta_graph_rag()\n",
    "\n",
    "# HTP 그림 검사 예시 질문\n",
    "inputs = {\n",
    "    \"original_question\": \"집에 창문은 2개 존재하고 크기는 적절함. 문은 집의 크기에 비해 작으며 문과 바깥이 길로 이어져 있지 않음.\",\n",
    "    \"category\": \"집\",\n",
    "}\n",
    "\n",
    "interupt_inputs = {\n",
    "    \"original_question\": \"오늘의 날씨는? 대한민국의 수도는? 최근 야구 경기 결과는?\",\n",
    "    \"category\": \"집\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e8a834",
   "metadata": {},
   "source": [
    "# Naive RAG time check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e10160d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 2. 질문 분해 시작 ---\n",
      "분해된 질문: ['오늘의 날씨에 대한 질문이 있다.', '대한민국의 수도에 대한 질문이 있다.', '최근 야구 경기 결과에 대한 질문이 있다.']\n",
      "Execution time for decompose_query_node: 1.20 seconds\n",
      "--- 3. 정보 검색 시작 (카테고리: 집) ---\n",
      "  - 검색 쿼리: '오늘의 날씨에 대한 질문이 있다.'\n",
      "  - 검색 쿼리: '대한민국의 수도에 대한 질문이 있다.'\n",
      "  - 검색 쿼리: '최근 야구 경기 결과에 대한 질문이 있다.'\n",
      "검색된 해석 Context 수: 11\n",
      "Execution time for retrieve_node: 1.17 seconds\n",
      "--- 4. 답변 생성 시작 ---\n",
      "생성된 답변 일부: ## 최종 해석 보고서\n",
      "\n",
      "본 보고서는 내담자의 HTP 그림 심리검사 결과를 토대로 심리상태를 종합적으로 분석한 내용입니다. 상담사가 관찰한 그림 특징을  해석 참고자료에 기반하여 논리적으로 연결하여 설명하겠습니다.\n",
      "\n",
      "### 1. 그림 크기\n",
      "내담자가 그림을 지나치게 작게 그렸다면, 이는 내면에 열등감이나 부적절감을 느끼고 있다는 가능성을 제시합니다. 이는 자...\n",
      "Execution time for generate_node: 21.84 seconds\n",
      "--- Individual Node Execution Times ---\n",
      "- decompose_query_node: 1.1987 seconds\n",
      "- retrieve_node: 1.1719 seconds\n",
      "- generate_node: 21.8375 seconds\n",
      "\n",
      "-----------------------------------------\n",
      "Total execution time (sum of nodes): 24.2081 seconds\n"
     ]
    }
   ],
   "source": [
    "# 시간 초기화\n",
    "execution_times = {}\n",
    "# 노드수행\n",
    "config = {\n",
    "    \"configurable\": {\n",
    "        \"faiss_store\": faiss_store, # 초기화 단계에서 생성한 faiss_store 객체\n",
    "        \"bm25_retriever_cache\": bm25_retriever_cache # 초기화 단계에서 생성한 bm25_retriever_cache 객체\n",
    "    }\n",
    "}\n",
    "naive_app.invoke(interupt_inputs, config=config)\n",
    "\n",
    "# --- 최종 결과 분석 ---\n",
    "print(\"--- Individual Node Execution Times ---\")\n",
    "for node_name, duration in execution_times.items():\n",
    "    print(f\"- {node_name}: {duration:.4f} seconds\")\n",
    "\n",
    "total_time = sum(execution_times.values())\n",
    "print(\"\\n-----------------------------------------\")\n",
    "print(f\"Total execution time (sum of nodes): {total_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c510ae",
   "metadata": {},
   "source": [
    "# Graph RAG time check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "2434d42a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1. 질문 관련성 검사 시작 ---\n",
      "질문 관련성: yes\n",
      "Execution time for relevance_check_node: 0.62 seconds\n",
      "결과: 관련성 있음. 질문 분해를 진행합니다.\n",
      "--- 2. 질문 분해 시작 ---\n",
      "분해된 질문: ['집 창문의 개수는 2개이고 크기는 적절하다.', '집 문의 크기가 집 전체에 비해 작다.', '집과 외부를 잇는 길이 그려져 있지 않다.']\n",
      "Execution time for decompose_query_node: 1.37 seconds\n",
      "--- 3. 정보 검색 시작 ---\n",
      "  - 검색 쿼리: '집 창문의 개수는 2개이고 크기는 적절하다.'\n",
      "  - 검색 쿼리: '집 문의 크기가 집 전체에 비해 작다.'\n",
      "  - 검색 쿼리: '집과 외부를 잇는 길이 그려져 있지 않다.'\n",
      "검색된 해석 Context 수: 12\n",
      "Execution time for retrieve_node: 1.28 seconds\n",
      "--- 4. 답변 생성 시작 ---\n",
      "생성된 답변 일부: ### 최종 해석 보고서\n",
      "\n",
      "이번 그림 심리검사의 분석 결과, 내담자의 심리상태를 다음과 같이 해석할 수 있습니다.\n",
      "\n",
      "**1. 집의 구조 (창문과 문):**\n",
      "내담자의 집에는 2개의 적절한 크기의 창문이 존재하지만, 문은 집의 크기에 비해 작고 바깥과 연결되는 길이 없습니다. 이 구조는 내담자가 중요한 대인관계와 외부 세상과의 연결에 어려움을 느끼고 있음을 나...\n",
      "Execution time for generate_node: 17.86 seconds\n",
      "--- 5. 환각 검사 시작 ---\n",
      "환각 검사 결과: yes\n",
      "Execution time for hallucination_check_node: 0.74 seconds\n",
      "결과: 환각 없음. 최종 답변을 반환합니다.\n",
      "--- Individual Node Execution Times ---\n",
      "- relevance_check_node: 0.6187 seconds\n",
      "- decompose_query_node: 1.3717 seconds\n",
      "- retrieve_node: 1.2847 seconds\n",
      "- generate_node: 17.8612 seconds\n",
      "- hallucination_check_node: 0.7357 seconds\n",
      "\n",
      "-----------------------------------------\n",
      "Total execution time (sum of nodes): 21.8720 seconds\n"
     ]
    }
   ],
   "source": [
    "# 시간 초기화\n",
    "execution_times = {}\n",
    "# 노드수행\n",
    "config = {\n",
    "    \"configurable\": {\n",
    "        \"faiss_store\": faiss_store, # 초기화 단계에서 생성한 faiss_store 객체\n",
    "        \"bm25_retriever_cache\": bm25_retriever_cache # 초기화 단계에서 생성한 bm25_retriever_cache 객체\n",
    "    }\n",
    "}\n",
    "graph_result = graph_app.invoke(inputs, config=config)\n",
    "\n",
    "# --- 최종 결과 분석 ---\n",
    "print(\"--- Individual Node Execution Times ---\")\n",
    "for node_name, duration in execution_times.items():\n",
    "    print(f\"- {node_name}: {duration:.4f} seconds\")\n",
    "\n",
    "total_time = sum(execution_times.values())\n",
    "print(\"\\n-----------------------------------------\")\n",
    "print(f\"Total execution time (sum of nodes): {total_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adaa6913",
   "metadata": {},
   "source": [
    "# Meta Graph RAG time check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "2c20ab37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1. 질문 관련성 검사 시작 ---\n",
      "질문 관련성: yes\n",
      "Execution time for relevance_check_node: 0.57 seconds\n",
      "결과: 관련성 있음. 질문 분해를 진행합니다.\n",
      "--- 2. 질문 분해 시작 ---\n",
      "분해된 질문: ['집 창문의 개수는 2개이고 크기는 적절하다.', '집 문의 크기가 집 전체에 비해 작다.', '문과 외부를 잇는 길이 그려져 있지 않다.']\n",
      "Execution time for decompose_query_node: 1.48 seconds\n",
      "--- 3. 정보 검색 시작 ---\n",
      "  - 검색 쿼리: '집 창문의 개수는 2개이고 크기는 적절하다.'\n",
      "  - 검색 쿼리: '집 문의 크기가 집 전체에 비해 작다.'\n",
      "  - 검색 쿼리: '문과 외부를 잇는 길이 그려져 있지 않다.'\n",
      "검색된 해석 Context 수: 13\n",
      "Execution time for retrieve_node: 1.80 seconds\n",
      "--- 4. 답변 생성 시작 ---\n",
      "생성된 답변 일부: ## 최종 해석 보고서\n",
      "\n",
      "내담자가 그린 집 그림을 분석한 결과에 따르면, 집의 구조와 요소들이 내담자의 심리적 상태와 대인 관계의 특성을 반영하고 있습니다.\n",
      "\n",
      "1. **창문의 수와 크기**: 내담자가 그린 창문은 2개로, 크기는 적절합니다. 이는 그가 현재 대인관계에서 적절한 수준의 개방성을 유지하고 있다는 것을 의미합니다. 그러나 대인관계에 대한 적극적인...\n",
      "Execution time for generate_node: 12.73 seconds\n",
      "--- 5. 환각 검사 시작 ---\n",
      "환각 검사 결과: yes\n",
      "Execution time for hallucination_check_node: 0.72 seconds\n",
      "결과: 환각 없음. 최종 답변을 반환합니다.\n",
      "--- Individual Node Execution Times ---\n",
      "- relevance_check_node: 0.5671 seconds\n",
      "- decompose_query_node: 1.4779 seconds\n",
      "- retrieve_node: 1.7958 seconds\n",
      "- generate_node: 12.7328 seconds\n",
      "- hallucination_check_node: 0.7153 seconds\n",
      "\n",
      "-----------------------------------------\n",
      "Total execution time (sum of nodes): 17.2889 seconds\n"
     ]
    }
   ],
   "source": [
    "# 시간 초기화\n",
    "execution_times = {}\n",
    "config = {\n",
    "    \"configurable\": {\n",
    "        \"faiss_store\": faiss_store, # 초기화 단계에서 생성한 faiss_store 객체\n",
    "        \"bm25_retriever_cache\": bm25_retriever_cache # 초기화 단계에서 생성한 bm25_retriever_cache 객체\n",
    "    }\n",
    "}\n",
    "# 노드수행\n",
    "\n",
    "meta_graph_result = graph_app.invoke(inputs, config=config)\n",
    "\n",
    "# --- 최종 결과 분석 ---\n",
    "print(\"--- Individual Node Execution Times ---\")\n",
    "for node_name, duration in execution_times.items():\n",
    "    print(f\"- {node_name}: {duration:.4f} seconds\")\n",
    "\n",
    "total_time = sum(execution_times.values())\n",
    "print(\"\\n-----------------------------------------\")\n",
    "print(f\"Total execution time (sum of nodes): {total_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce42b99",
   "metadata": {},
   "source": [
    "# LLM as a judge\n",
    "평가자 gpt-4o\n",
    "[https://galtea.ai/2025/05/02/evaluation-of-judges.html]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f66f821",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
