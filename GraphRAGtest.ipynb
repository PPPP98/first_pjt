{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce3abde4",
   "metadata": {},
   "source": [
    "# LangGraph test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9f7e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langgraph dotenv langchain langchain-openai langchain-community openai faiss-cpu rank_bm25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57088542",
   "metadata": {},
   "source": [
    "# set env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34b6d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ[\"OPENAI_API_BASE\"] = os.getenv(\"OPENAI_API_BASE\")\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "faiss_store = (\n",
    "    FAISS.load_local(\n",
    "        \"faiss_store\", embeddings=embeddings, allow_dangerous_deserialization=True\n",
    "    )\n",
    "    if os.path.exists(\"faiss_store\")\n",
    "    else None\n",
    ")\n",
    "all_texts = [doc.page_content for doc in faiss_store.docstore._dict.values()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3574d6",
   "metadata": {},
   "source": [
    "# FAISS + ensemble[bm25 / similarity]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1e576c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.retrievers import BM25Retriever\n",
    "from langchain.retrievers import EnsembleRetriever\n",
    "\n",
    "K = 3\n",
    "weights = [0.3, 0.7]\n",
    "\n",
    "# Sparse\n",
    "sparse_bm25_retriever = BM25Retriever.from_texts(texts=all_texts)\n",
    "sparse_bm25_retriever.k = K  # k값을 통일하여 설정\n",
    "# Dense\n",
    "dense_similarity_retriever = faiss_store.as_retriever(\n",
    "    search_type=\"similarity\", search_kwargs={\"k\": K}\n",
    ")\n",
    "\n",
    "retriever = EnsembleRetriever(\n",
    "    retrievers=[sparse_bm25_retriever, dense_similarity_retriever],\n",
    "    weights=weights,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43dd2dfd",
   "metadata": {},
   "source": [
    "# state 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00c52f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, TypedDict\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Graph RAG 파이프라인의 상태\n",
    "\n",
    "    Attributes:\n",
    "        original_question (str): 사용자가 입력한 원본 질문 (그림 묘사)\n",
    "        decomposed_questions (List[str]): 의미 단위로 분해된 질문 리스트\n",
    "        retrieved_contexts (List[str]): 검색된 관련 문서(해석) 조각 리스트\n",
    "        generation (str): LLM이 생성한 최종 해석\n",
    "        relevance_check (str): 질문의 HTP 검사 관련성 여부 (\"yes\" or \"no\")\n",
    "        hallucination_check (str): 생성된 답변의 환각 현상 유무 (\"yes\" or \"no\")\n",
    "        category (str): 질문 범주(집, 나무, 사람)\n",
    "    \"\"\"\n",
    "    original_question: str\n",
    "    decomposed_questions: List[str]\n",
    "    retrieved_contexts: List[str]\n",
    "    generation: str\n",
    "    relevance_check: str\n",
    "    hallucination_check: str\n",
    "    category: str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cad33d1",
   "metadata": {},
   "source": [
    "# 노드 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9091b8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser, JsonOutputParser\n",
    "\n",
    "# 2-1. Relevance Check Node (질문 관련성 검사)\n",
    "def relevance_check_node(state: GraphState):\n",
    "    \"\"\"\n",
    "    입력된 질문이 HTP 심리검사 해석과 관련이 있는지 확인합니다.\n",
    "    \"\"\"\n",
    "    print(\"--- 1. 질문 관련성 검사 시작 ---\")\n",
    "    question = state[\"original_question\"]\n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_template(\n",
    "        \"\"\"당신은 심리검사 전문가입니다. 주어진 질문이 'HTP(집-나무-사람) 그림 심리검사' 해석과 관련된 내용인지 판단해주세요.\n",
    "        HTP 검사는 집, 나무, 사람 그림의 특징(예: 지붕, 문, 창문, 나무 기둥, 가지, 사람의 눈, 코, 입 등)을 분석하는 것입니다.\n",
    "        질문은 HTP 그림에 대한 관찰 묘사로, 그림의 요소나 특징에 대한 내용입니다.\n",
    "        이에 해당하면 'yes', 전혀 관련 없는 내용(예: 오늘 날씨, 스포츠 경기 결과 등)이면 'no'로만 대답해주세요.\n",
    "\n",
    "        질문: {question}\n",
    "        판단 (yes/no):\"\"\"\n",
    "    )\n",
    "    \n",
    "    chain = prompt | llm | StrOutputParser()\n",
    "    relevance = chain.invoke({\"question\": question})\n",
    "    \n",
    "    print(f\"질문 관련성: {relevance}\")\n",
    "    state[\"relevance_check\"] = relevance.lower()\n",
    "    if state[\"relevance_check\"] == \"no\":\n",
    "        # print(\"질문이 HTP 검사와 관련이 없습니다. 프로세스를 종료합니다.\")\n",
    "        state[\"generation\"] = \"관찰 결과를 다시 입력해주세요.\"\n",
    "        print(state)\n",
    "    return state\n",
    "\n",
    "def decompose_query_node(state: GraphState):\n",
    "    \"\"\"\n",
    "    입력된 질문을 의미 단위의 여러 하위 질문으로 분해합니다.\n",
    "    \"\"\"\n",
    "    print(\"--- 2. 질문 분해 시작 ---\")\n",
    "    question = state[\"original_question\"]\n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_template(\n",
    "        \"\"\"당신은 HTP 그림 심리검사 문장 분석 전문가입니다. 상담사가 그림을 보고 관찰한 내용을 나열한 문장이 주어집니다. \n",
    "        이 문장을 그림의 각 요소(예: 문, 창문, 지붕, 길 등)에 대한 독립적인 해석이 가능한 단위로 분해하여 JSON 리스트 형태로 반환해주세요.\n",
    "\n",
    "        예시:\n",
    "        입력: \"집에 창문은 2개 존재하고 크기는 적절함. 문은 집의 크기에 비해 작으며 문과 바깥이 길로 이어져 있지 않음.\"\n",
    "        출력: {{\"queries\": [\"집 창문의 개수는 2개이고 크기는 적절하다.\", \"집 문의 크기가 집 전체에 비해 작다.\", \"집과 외부를 잇는 길이 그려져 있지 않다.\"]}}\n",
    "\n",
    "        입력: {question}\n",
    "        출력:\"\"\"\n",
    "    )\n",
    "    \n",
    "    chain = prompt | llm | JsonOutputParser()\n",
    "    decomposed = chain.invoke({\"question\": question})\n",
    "    \n",
    "    decomposed_questions = decomposed.get(\"queries\", [])\n",
    "    print(f\"분해된 질문: {decomposed_questions}\")\n",
    "    state[\"decomposed_questions\"] = decomposed_questions\n",
    "    return state\n",
    "\n",
    "# 2-3. Retrieve Node (정보 검색)\n",
    "def retrieve_node(state: GraphState):\n",
    "    \"\"\"\n",
    "    분해된 각 질문에 대해 Ensemble Retriever를 사용하여 관련 문서를 검색합니다.\n",
    "    \"\"\"\n",
    "    print(\"--- 3. 정보 검색 시작 ---\")\n",
    "    decomposed_questions = state[\"decomposed_questions\"]\n",
    "    all_retrieved_docs = []\n",
    "\n",
    "    for query in decomposed_questions:\n",
    "        print(f\"  - 검색 쿼리: '{query}'\")\n",
    "        # 여기서 사용자가 제공한 retriever를 사용합니다.\n",
    "        retrieved_docs = retriever.invoke(query)\n",
    "        \n",
    "        # 검색 결과의 내용을 문자열로 변환하여 추가\n",
    "        doc_texts = [doc.page_content for doc in retrieved_docs]\n",
    "        all_retrieved_docs.extend(doc_texts)\n",
    "    \n",
    "    # 중복 제거\n",
    "    unique_contexts = list(set(all_retrieved_docs))\n",
    "    print(f\"검색된 해석 Context 수: {len(unique_contexts)}\")\n",
    "    state[\"retrieved_contexts\"] = unique_contexts\n",
    "    return state\n",
    "\n",
    "# 2-4. Generate Node (답변 생성)\n",
    "def generate_node(state: GraphState):\n",
    "    \"\"\"\n",
    "    검색된 Context를 바탕으로 최종 해석 답변을 생성합니다.\n",
    "    \"\"\"\n",
    "    print(\"--- 4. 답변 생성 시작 ---\")\n",
    "    question = state[\"original_question\"]\n",
    "    contexts = \"\\n\\n\".join(state[\"retrieved_contexts\"])\n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_template(\n",
    "        \"\"\"당신은 HTP 그림 심리검사 결과 해석 전문가입니다.\n",
    "        상담사가 관찰한 '그림 특징'과 그에 대한 '해석 참고자료'가 주어집니다. \n",
    "        두 정보를 종합하여, 내담자의 심리상태에 대한 최종 해석 보고서를 작성해주세요.\n",
    "        전문적이고 이해하기 쉬운 말투로 설명하고, 각 특징과 해석을 논리적으로 연결하여 설명해주세요.\n",
    "        정보는 반드시 주어진 '해석 참고자료'에 근거해야 하며, 환각(hallucination)이 없어야 합니다.\n",
    "        답변은 질문에 대한 관찰 내용과 관련된 것만 작성해야 합니다.\n",
    "        각 특징에 대한 해석 내용을 분석하고 참고 자료에서 관련 내용을 찾아 종합적으로 해석하세요.\n",
    "        답변은 너무 극단적이지 않고 충분히 납득할 수 있는 수준으로 작성해야 합니다.\n",
    "        참고자료가 너무 적거나 관련성이 낮다면, 답변은 간단하게 작성해주세요.\n",
    "\n",
    "        ## 상담사의 그림 특징 관찰 내용:\n",
    "        {question}\n",
    "\n",
    "        ## 해석 참고자료:\n",
    "        {context}\n",
    "\n",
    "        ## 최종 해석 보고서:\n",
    "        \"\"\"\n",
    "    )\n",
    "    \n",
    "    chain = prompt | llm | StrOutputParser()\n",
    "    generation = chain.invoke({\"question\": question, \"context\": contexts})\n",
    "    \n",
    "    print(\"생성된 답변 일부:\", generation[:200] + \"...\")\n",
    "    state[\"generation\"] = generation\n",
    "    return state\n",
    "\n",
    "# 2-5. Hallucination Check Node (환각 검사)\n",
    "def hallucination_check_node(state: GraphState):\n",
    "    \"\"\"\n",
    "    생성된 답변에 환각(hallucination)이 있는지 검사합니다.\n",
    "    \"\"\"\n",
    "    print(\"--- 5. 환각 검사 시작 ---\")\n",
    "    contexts = state[\"retrieved_contexts\"]\n",
    "    generation = state[\"generation\"]\n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_template(\n",
    "        \"\"\"당신은 AI 답변 검증 전문가입니다. 주어진 '참고 자료'를 바탕으로 '생성된 답변'이 만들어졌는지 확인해야 합니다.\n",
    "        '생성된 답변'의 모든 내용이 '참고 자료'에 근거하고 있다면 'yes'를, '참고 자료'에 없는 내용이 포함되어 있다면 'no'를 반환해주세요.\n",
    "\n",
    "        ## 참고 자료:\n",
    "        {context}\n",
    "\n",
    "        ## 생성된 답변:\n",
    "        {generation}\n",
    "\n",
    "        판단 (yes/no):\"\"\"\n",
    "    )\n",
    "    \n",
    "    chain = prompt | llm | StrOutputParser()\n",
    "    check_result = chain.invoke({\"context\": \"\\n\\n\".join(contexts), \"generation\": generation})\n",
    "\n",
    "    print(f\"환각 검사 결과: {check_result}\")\n",
    "    state[\"hallucination_check\"] = check_result.lower()\n",
    "    return state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af20e5c4",
   "metadata": {},
   "source": [
    "# Edge 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30bccccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3-1. 질문 관련성 검사 후 분기\n",
    "def decide_after_relevance_check(state: GraphState):\n",
    "    \"\"\"\n",
    "    질문 관련성 검사 결과에 따라 다음 단계를 결정합니다.\n",
    "    - \"yes\": 질문 분해 단계로 이동\n",
    "    - \"no\": 종료\n",
    "    \"\"\"\n",
    "    if state[\"relevance_check\"] == \"yes\":\n",
    "        print(\"결과: 관련성 있음. 질문 분해를 진행합니다.\")\n",
    "        return \"decompose\"\n",
    "    else:\n",
    "        print(\"결과: 관련성 없음. 프로세스를 종료합니다.\")\n",
    "        return \"end\"\n",
    "\n",
    "# 3-2. 환각 검사 후 분기\n",
    "def decide_after_hallucination_check(state: GraphState):\n",
    "    \"\"\"\n",
    "    환각 검사 결과에 따라 다음 단계를 결정합니다.\n",
    "    - \"yes\": 환각 없음, 종료\n",
    "    - \"no\": 환각 존재, 답변 재생성\n",
    "    \"\"\"\n",
    "    if state[\"hallucination_check\"] == \"yes\":\n",
    "        print(\"결과: 환각 없음. 최종 답변을 반환합니다.\")\n",
    "        # print(state)\n",
    "        return \"end\"\n",
    "    else:\n",
    "        print(\"결과: 환각 존재. 답변을 다시 생성합니다.\")\n",
    "        return \"regenerate\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5d1668",
   "metadata": {},
   "source": [
    "# 그래프 통합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de444f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END, StateGraph\n",
    "\n",
    "# 그래프 빌더 생성\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# 노드 추가\n",
    "workflow.add_node(\"relevance_check\", relevance_check_node)\n",
    "workflow.add_node(\"decompose_query\", decompose_query_node)\n",
    "workflow.add_node(\"retrieve\", retrieve_node)\n",
    "workflow.add_node(\"generate\", generate_node)\n",
    "workflow.add_node(\"hallucination_check\", hallucination_check_node)\n",
    "\n",
    "# 엣지 연결\n",
    "workflow.set_entry_point(\"relevance_check\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"relevance_check\",\n",
    "    decide_after_relevance_check,\n",
    "    {\"decompose\": \"decompose_query\", \"end\": END}\n",
    ")\n",
    "workflow.add_edge(\"decompose_query\", \"retrieve\")\n",
    "workflow.add_edge(\"retrieve\", \"generate\")\n",
    "workflow.add_edge(\"generate\", \"hallucination_check\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"hallucination_check\",\n",
    "    decide_after_hallucination_check,\n",
    "    {\"regenerate\": \"generate\", \"end\": END}\n",
    ")\n",
    "\n",
    "# 그래프 컴파일\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4069d161",
   "metadata": {},
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07e3708",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HTP 그림 검사 예시 질문\n",
    "inputs = {\n",
    "    \"original_question\": \"집에 창문은 2개 존재하고 크기는 적절함. 문은 집의 크기에 비해 작으며 문과 바깥이 길로 이어져 있지 않음.\"\n",
    "}\n",
    "\n",
    "# # 그래프 실행\n",
    "# for output in app.stream(inputs, {\"recursion_limit\": 5}): # 순환 방지를 위해 recursion_limit 설정\n",
    "#     for key, value in output.items():\n",
    "#         # 각 단계의 최종 출력만 표시\n",
    "#         print(f\"노드 '{key}' 완료:\")\n",
    "#         # print(f\"  - 상태: {value}\") # 전체 상태를 보려면 주석 해제\n",
    "#         print(\"---\")\n",
    "\n",
    "# 최종 결과 확인\n",
    "final_state = app.invoke(inputs, {\"recursion_limit\": 20})\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"          최종 HTP 심리검사 해석 결과\")\n",
    "print(\"=\"*50)\n",
    "print(final_state['generation'])\n",
    "print(\"=\"*50)\n",
    "\n",
    "# # 관련 없는 질문 예시\n",
    "# print(\"\\n\\n\" + \"=\"*50)\n",
    "# print(\"          관련 없는 질문 테스트\")\n",
    "# print(\"=\"*50)\n",
    "# inputs_irrelevant = {\"original_question\": \"오늘 프로야구 경기 결과는 어떻게 되나요?\"}\n",
    "# irrelevant_result = app.invoke(inputs_irrelevant)\n",
    "# print(\"최종 상태:\", irrelevant_result)\n",
    "# print(\"=\"*50)\n",
    "# print(\"생성된 답변:\", irrelevant_result['generation'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
